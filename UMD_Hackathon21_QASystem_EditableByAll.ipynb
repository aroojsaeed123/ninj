{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UMD_Hackathon21_QASystem_EditableByAll.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcKrFT0JLMky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "996517be-e010-44b4-add5-c50504be7ef4"
      },
      "source": [
        "#BLOCK 1\n",
        "#Creates a folder for our Project in Google Colab\n",
        "!mkdir umd_hackathon2021\n",
        "%cd umd_hackathon2021"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/umd_hackathon2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z4lgn8rMXvU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ce17eff-d04a-481a-cf8f-6dcbea1db89e"
      },
      "source": [
        "#BLOCK 2 \n",
        "#Makefile script downloads training, testing and development datasets from servers of Amazon Web Services\n",
        "%%writefile /content/umd_hackathon2021/Makefile\n",
        "qanta.train.json qanta.test.json qanta.dev.json:\n",
        "\twget https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/qanta.train.2018.04.18.json\n",
        "\tmv qanta.train.2018.04.18.json qanta.train.json\n",
        "\twget https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/qanta.dev.2018.04.18.json\n",
        "\tmv qanta.dev.2018.04.18.json qanta.dev.json\n",
        "\twget https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/qanta.test.2018.04.18.json\n",
        "\tmv qanta.test.2018.04.18.json qanta.test.json\n",
        "qanta.train.evidence.json qanta.dev.evidence.json qanta.test.evidence.json:\n",
        "\twget https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/rc/evidence_docs_train.json\n",
        "\tmv evidence_docs_train.json qanta.train.evidence.json\n",
        "\twget https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/rc/evidence_docs_dev.json\n",
        "\tmv evidence_docs_dev.json qanta.dev.evidence.json\n",
        "\twget https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/rc/evidence_docs_test.json\n",
        "\tmv evidence_docs_test.json qanta.test.evidence.json\n",
        "qanta.train.evidence.text.json qanta.dev.evidence.text.json qanta.test.evidence.sent.text.json:\n",
        "\twget https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/rc/evidence_docs_train_with_sent_text.json\n",
        "\tmv evidence_docs_train_with_sent_text.json qanta.train.evidence.text.json\n",
        "\twget https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/rc/evidence_docs_dev_with_sent_text.json\n",
        "\tmv evidence_docs_dev_with_sent_text.json qanta.dev.evidence.text.json\n",
        "\twget https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/rc/evidence_docs_test_with_sent_text.json\n",
        "\tmv evidence_docs_test_with_sent_text.json qanta.test.evidence.text.json\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing /content/umd_hackathon2021/Makefile\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0A8CATHyMXkk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6c2c944-6499-43e4-8db4-7ae9d1193248"
      },
      "source": [
        "#BLOCK 3\n",
        "#Downloads the relevant Datasets\n",
        "!make qanta.train.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wget https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/qanta.train.2018.04.18.json\n",
            "--2021-04-11 05:25:36--  https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/qanta.train.2018.04.18.json\n",
            "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.219.16\n",
            "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.219.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 142113194 (136M) [application/json]\n",
            "Saving to: ‘qanta.train.2018.04.18.json’\n",
            "\n",
            "qanta.train.2018.04 100%[===================>] 135.53M  36.5MB/s    in 4.3s    \n",
            "\n",
            "2021-04-11 05:25:41 (31.8 MB/s) - ‘qanta.train.2018.04.18.json’ saved [142113194/142113194]\n",
            "\n",
            "mv qanta.train.2018.04.18.json qanta.train.json\n",
            "wget https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/qanta.dev.2018.04.18.json\n",
            "--2021-04-11 05:25:41--  https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/qanta.dev.2018.04.18.json\n",
            "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.133.0\n",
            "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.133.0|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3090653 (2.9M) [application/json]\n",
            "Saving to: ‘qanta.dev.2018.04.18.json’\n",
            "\n",
            "qanta.dev.2018.04.1 100%[===================>]   2.95M  5.19MB/s    in 0.6s    \n",
            "\n",
            "2021-04-11 05:25:42 (5.19 MB/s) - ‘qanta.dev.2018.04.18.json’ saved [3090653/3090653]\n",
            "\n",
            "mv qanta.dev.2018.04.18.json qanta.dev.json\n",
            "wget https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/qanta.test.2018.04.18.json\n",
            "--2021-04-11 05:25:42--  https://s3-us-west-2.amazonaws.com/pinafore-us-west-2/qanta-jmlr-datasets/qanta.test.2018.04.18.json\n",
            "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.133.0\n",
            "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.133.0|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5873986 (5.6M) [application/json]\n",
            "Saving to: ‘qanta.test.2018.04.18.json’\n",
            "\n",
            "qanta.test.2018.04. 100%[===================>]   5.60M  8.26MB/s    in 0.7s    \n",
            "\n",
            "2021-04-11 05:25:43 (8.26 MB/s) - ‘qanta.test.2018.04.18.json’ saved [5873986/5873986]\n",
            "\n",
            "mv qanta.test.2018.04.18.json qanta.test.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRpLF8LmMXaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4d438a8-2d6a-4909-9eba-dc433bb43361"
      },
      "source": [
        "#BLOCK 4\n",
        "#K-NN Classification code to classify answers from questions\n",
        "%%writefile /content/umd_hackathon2021/knn_hack.py\n",
        "\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import json\n",
        "from collections import Counter, defaultdict\n",
        "from typing import Sequence, Dict\n",
        "\n",
        "import numpy\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "class Knearest:\n",
        "    \"\"\"\n",
        "    kNN classifier\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, x, y, k):\n",
        "        \"\"\"\n",
        "        Creates a kNN instance\n",
        "        :param x: Training data input\n",
        "        :param y: Training data output\n",
        "        :param k: The number of nearest points to consider in classification\n",
        "        \"\"\"\n",
        "\n",
        "        # You may modify this code, but you shouldn't need to\n",
        "\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.k = k\n",
        "\n",
        "    def majority(self, item_indices: Sequence[int]) -> str:\n",
        "        \"\"\"Given the indices of training examples, return the majority label.\n",
        "        If there's a tie, return the one that is lexicographically\n",
        "        first (as determined by python sorted function).\n",
        "        :param item_indices: The indices of the k nearest neighbors\n",
        "        (helpfully, this is what's returned by the kneighbors\n",
        "        function.\n",
        "        \"\"\"\n",
        "        assert len(item_indices) == self.k, \"Did not get k inputs\"\n",
        "\n",
        "        # Finish this function to return the most common y value for\n",
        "        # these indices\n",
        "\n",
        "        # 1. counter on y\n",
        "        freq = Counter(self.y[index] for index in item_indices)\n",
        "        # 2. Find the most common y value using function provided by counter\n",
        "        return sorted(freq.most_common())[0][0]\n",
        "\n",
        "        #1 is okay\n",
        "        freq = Counter(self.y[index] for index in item_indices)\n",
        "        #get the count of the most common element (1) https://www.kite.com/python/docs/collections.Counter.most_common\n",
        "        max_count = freq.most_common(1)[0][1]\n",
        "        #sort the items in freq with a for loop looping on freq to make sure that we get the items where count[item_in_count]==the_count_of_the most_common_element\n",
        "        return sorted(i for i in freq if freq[i]  == max_count)[0]\n",
        "                   \n",
        "    def classify(self, example: numpy.ndarray) -> str:\n",
        "        \"\"\"\n",
        "        Given an example, classify the example.\n",
        "        :param example: A representation of an example in the same\n",
        "        format as training data\n",
        "        \"\"\"\n",
        "        # Finish this function to find the k closest points, query the\n",
        "        # majority function, and return the value.\n",
        "\n",
        "\n",
        "        # example : \n",
        "\n",
        "        # Self.x :\n",
        "\n",
        "        # 1. normalize example and training data\n",
        "        # 2. make them unit vectors\n",
        "        # 3. take their dot product. \n",
        "        # Take a dot product between example and example.T multiplying all elements by 0.5 \n",
        "        # ex.dot(ex.T)[0][0]**0.5 \n",
        "        #EXAMPLE QUESTION\n",
        "        # ex = numpy.linalg.norm(example)\n",
        "        # ex_unit = example/ex\n",
        "        # TRAINING DATASET\n",
        "        # x_norm = numpy.linalg.norm(self.x)\n",
        "        # x_unit = self.x/x_norm\n",
        "\n",
        "        # dot_prod = numpy.dot(x_unit, ex_unit.T)\n",
        "\n",
        "\n",
        "        # 1. **0.5 = ^1/2\n",
        "        #Normalization below will happen if example is a numpy.ndarray\n",
        "        # ex_norm = ex.dot(ex.T)[0][0]**0.5 \n",
        "        # But example is a csr_matrix data type\n",
        "        # code to attend numpy arrays in the test code is essential if we want to run the test code\n",
        "        # But we dont need to run the test code for now\n",
        "        # Let's directly focus on the code to attend to a non-numpy datatype object which is csr_matrix\n",
        "        # possible_similarities=self.x.dot(example.T).T\n",
        "        # a= []\n",
        "        # print(type(a))\n",
        "        '''\n",
        "        possible_similarities=self.x.dot(example.T).T # [1,2,3,4,4] or [[];[];[]]\n",
        "\n",
        "        possible_similarities = (possible_similarities).toarray()\n",
        " \n",
        "        ind = numpy.argsort(possible_similarities)\n",
        "        \n",
        "        # what we pass to majority. we'll acc the inds in here. \n",
        "        start = len(ind) - self.k\n",
        "        return self.majority(ind[start:])\n",
        "        '''\n",
        "        possible_similarities=self.x.dot(example.T).T # [1,2,3,4,4] or [[];[];[]]\n",
        "\n",
        "        possible_similarities = (-possible_similarities).toarray()\n",
        "       \n",
        "        # sorted in reverse i.e largest to smallest\n",
        "        # r_ps = (sorted(possible_similarities))[::-1]\n",
        "\n",
        "        ind = numpy.argsort(possible_similarities, axis=1)[:, 0:self.k]\n",
        "        \n",
        "        # what we pass to majority. we'll acc the inds in here. \n",
        "        return self.majority(ind[0])\n",
        "\n",
        "    def confusion_matrix(self, test_x: Sequence[str], test_y: Sequence[str]) -> Dict[str, Dict[str, int]]:\n",
        "        \"\"\"\n",
        "        Given a matrix of test examples and labels, compute the confusion\n",
        "        matrixfor the current classifier.  Should return a dictionary of\n",
        "        dictionaries where d[ii][jj] is the number of times an example\n",
        "        with true label ii was labeled as jj.\n",
        "        :param test_x: Test data representation\n",
        "        :param test_y: Test data answers\n",
        "        \"\"\"\n",
        "\n",
        "        # Finish this function to build a dictionary with the\n",
        "        # mislabeled examples.  You'll need to call the classify\n",
        "        # function for each example.\n",
        "\n",
        "\n",
        "        #zip() test_x and test_y to make set of pairs\n",
        "        #iterate over new zip object\n",
        "          #compare test_x with test_y (classify)\n",
        "          #if there is no dictionary definition, \n",
        "            #add one with labels and give (comparison value) to int value\n",
        "          #else\n",
        "            #add (comparison value) to the int value\n",
        "\n",
        "        d = defaultdict(dict)\n",
        "        index = 0\n",
        "        \n",
        "        for tx, ty in zip(text_x, test_y):\n",
        "          output_label = classify(tx)\n",
        "          d[yy][guess] = d[ty].get(output_label, 0) + 1\n",
        "          index += 1\n",
        "          if (index % 100 == 0):\n",
        "            print(\"%i/%i Confusion Matrix\" % (index, test_x.shape[0]))\n",
        "          \n",
        "        return d\n",
        "\n",
        "\n",
        "            \n",
        "\n",
        "    @staticmethod\n",
        "    def accuracy(confusion_matrix: Dict[str, Dict[str, int]]) -> float:\n",
        "        \"\"\"Given a confusion matrix, compute the accuracy of the underlying\n",
        "        classifier.\n",
        "        \"\"\"\n",
        "\n",
        "        # Hint: this should give you clues as to how the confusion\n",
        "        # matrix should be structured.\n",
        "\n",
        "        total = 0 \n",
        "        correct = 0\n",
        "        for ii in confusion_matrix:\n",
        "            total += sum(confusion_matrix[ii].values())\n",
        "            correct += confusion_matrix[ii].get(ii, 0)\n",
        "\n",
        "        return float(correct) / float(total)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description='KNN classifier options')\n",
        "    parser.add_argument(\"--root_dir\", help=\"QB Dataset for training\",\n",
        "                        type=str, default='../',\n",
        "                        required=False)\n",
        "    parser.add_argument(\"--train_dataset\", help=\"QB Dataset for training\",\n",
        "                        type=str, default='/content/umd_hackathon2021/qanta.train.json',\n",
        "                        required=False)\n",
        "    parser.add_argument(\"--test_dataset\", help=\"QB Dataset for test\",\n",
        "                        type=str, default='/content/umd_hackathon2021/qanta.dev.json',\n",
        "                        required=False)\n",
        "    parser.add_argument(\"--min_df\", help=\"How many documents must a word appear in to be feature\",\n",
        "                        type=int, default=2)\n",
        "    parser.add_argument(\"--max_df\", help=\"How many docs can words appear in and still be feature\",\n",
        "                        type=float, default=0.9)\n",
        "    parser.add_argument(\"--limit\", help=\"Number of training documents\",\n",
        "                        type=int, default=-1, required=False)\n",
        "    parser.add_argument(\"--max_ngram\", help=\"Max ngram length\", type=int, default=3)\n",
        "    parser.add_argument('--k', type=int, default=3,\n",
        "                        help=\"Number of nearest points to use\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # You should not have to modify any of this code\n",
        "    with open(os.path.join(args.root_dir, args.train_dataset)) as infile:\n",
        "        data = json.load(infile)[\"questions\"]\n",
        "        if args.limit > 0:\n",
        "            data = data[:args.limit]\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(1, args.max_ngram), min_df=args.min_df, max_df=args.max_df).fit(x[\"text\"] for x in data)\n",
        "    train_x = vectorizer.transform(x[\"text\"] for x in data)\n",
        "    train_y = list(x[\"page\"] for x in data)\n",
        "\n",
        "    print(type(train_x))\n",
        "\n",
        "    knn = Knearest(train_x, train_y, args.k)\n",
        "    print(\"Done loading data\")\n",
        "\n",
        "    with open(os.path.join(args.root_dir, args.test_dataset)) as infile:\n",
        "        test = json.load(infile)[\"questions\"][:100]\n",
        "\n",
        "    test_x = vectorizer.transform(x[\"text\"] for x in test)\n",
        "    test_y = list(x[\"page\"] for x in test)\n",
        "    answers = [x[0] for x in Counter(test_y).most_common(5)]\n",
        "\n",
        "    confusion = knn.confusion_matrix(test_x, test_y)\n",
        "    guesses = set()\n",
        "    for ii in answers:\n",
        "        for jj in confusion[ii]:\n",
        "            guesses.add(jj)\n",
        "\n",
        "    print(\"\\t\" + \"\\t\".join(str(x) for x in answers))\n",
        "    print(\"\".join([\"-\"] * 90))\n",
        "    for ii in guesses:\n",
        "        print(\"%30s:\\t\" % ii + \"\\t\".join(str(confusion[x].get(ii, 0))\n",
        "                                       for x in answers))\n",
        "    print(\"Accuracy: %f\" % knn.accuracy(confusion))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing /content/umd_hackathon2021/knn_hack.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIFu_QhWMXP_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dea07226-1fcd-486e-ee92-d2ce7e282f18"
      },
      "source": [
        "#BLOCK 5\n",
        "#Test Code to see if the K-NN Classification code works as expected\n",
        "#Note: Test codes are always useful for debugging purposes\n",
        "%%writefile /content/umd_hackathon2021/test_hack.py\n",
        "\n",
        "import unittest\n",
        "\n",
        "from numpy import array\n",
        "\n",
        "from knn_hack import *\n",
        "\n",
        "class TestKnn(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        self.x = array([[2, 0], [4, 1], [6, 0], [1, 4], [2, 4], [2, 5], [4, 4],\n",
        "                        [0, 2], [3, 2], [4, 2], [5, 2], [7, 3], [5, 5]])\n",
        "        self.y = array([+1, +1, +1, +1, +1, +1, +1, -1, -1, -1, -1, -1, -1])\n",
        "        self.knn = {}\n",
        "        for ii in [1, 2, 3]:\n",
        "            self.knn[ii] = Knearest(self.x, self.y, ii)\n",
        "\n",
        "        self.queries = [array(x).reshape(1, -1) for x in\n",
        "                        [[1, 5], [0, 3], [6, 1], [6, 4]]]\n",
        "        self.test_y = [1, -1, 1, -1]\n",
        "\n",
        "    def test1(self):\n",
        "        self.assertEqual(self.knn[1].classify(self.queries[0]), 1)\n",
        "        self.assertEqual(self.knn[1].classify(self.queries[1]), -1)\n",
        "        self.assertEqual(self.knn[1].classify(self.queries[2]), 1)\n",
        "        self.assertEqual(self.knn[1].classify(self.queries[3]), -1)\n",
        "        self.assertEqual(self.knn[1].accuracy(self.knn[1].confusion_matrix(self.queries, self.test_y)), 1.0)\n",
        "\n",
        "    def test2(self):\n",
        "        self.assertEqual(self.knn[2].classify(self.queries[0]), 1)\n",
        "        self.assertEqual(self.knn[2].classify(self.queries[1]), -1)\n",
        "        self.assertEqual(self.knn[2].classify(self.queries[2]), 1)\n",
        "        self.assertEqual(self.knn[2].classify(self.queries[3]), -1)\n",
        "        self.assertEqual(self.knn[2].accuracy(self.knn[2].confusion_matrix(self.queries, self.test_y)), 1.0)\n",
        "\n",
        "\n",
        "    def test3(self):\n",
        "        self.assertEqual(self.knn[3].classify(self.queries[0]), 1)\n",
        "        self.assertEqual(self.knn[3].classify(self.queries[1]), 1)\n",
        "        self.assertEqual(self.knn[3].classify(self.queries[2]), 1)\n",
        "        self.assertEqual(self.knn[3].classify(self.queries[3]), -1)\n",
        "        self.assertEqual(self.knn[3].accuracy(self.knn[3].confusion_matrix(self.queries, self.test_y)), 0.75)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/umd_hackathon2021/test_hack.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19Aq1nrTMXFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e0d89b8-3534-4960-f249-9e660448af2f"
      },
      "source": [
        "#BLOCK 6\n",
        "#Executes the test code\n",
        "!python test_hack.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python3: can't open file 'test_hack.py': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBzvNhQbMWZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b7d097f-e0d3-425b-d2f6-def455b64d19"
      },
      "source": [
        "#BLOCK 7\n",
        "#Executes the K-NN Classification algorithm\n",
        "!python knn_hack.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python3: can't open file 'knn_hack.py': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Cqacg3mTmy1"
      },
      "source": [
        "#Block 8 (Buzz for answers above a threshold value and generate a csv file adapting knn_hack.py in Block 4)\n",
        "# Calculate confidence scores for answers generated for each sentence \n",
        "'''For each sentence in a question\n",
        "get the corresponding answer (an answer document that is vectorized)\n",
        "take a dot product of the original question representation with the generated answer representation for that question\n",
        "This dot product will be the confidence scores'''\n",
        "\n",
        "# Threshold on the confidence scores; buzz = 1 for confidence scores above threshold otherwise 0\n",
        "# store this data in a csv file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlXO7Ki4p4jr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jx51Sn-qCFt"
      },
      "source": [
        "{\"text\": \"The lines \\\"Rude, and ungrateful, though my country be,\\\" in Book 5 of this work alludes to Scipio Africanus as a reference to its author's multiple exiles. The main characters of this work encounter a figure whose \\\"black lips\\\" part to reveal \\\"blue rows\\\" of \\\"gnashing teeth.\\\" Its central character is brought to a mountaintop and listens to a set of speeches by the Nereid Thetis after arriving at the Isle of Joy. This work concludes with an apostrophe to the king of a nation whose history is recounted to the King of Melinda in the third, fourth, and fifth books of this work. A character in this work who used to rule over the Indian Ocean first appears as a storm and guards the Cape of Good Hope. Bacchus and Venus feud over a group of sailors in this work who are impeded by the monster Adamastor. For 10 points, name this Portuguese epic by Luis de Cam\\u00f5es (\\\"ka-MOISH\\\") chronicling the voyages of Vasco da Gama.\", \n",
        "\"answer\": \"The Lusiads [or Os Lusiadas] &lt;Miscellaneous Lit&gt; Bonuses\", \n",
        "\"page\": \"Os_Lus\\u00edadas\", \n",
        "\"category\": \"Literature\", \n",
        "\"subcategory\": \"European\", \n",
        "\"tournament\": \"ACF Regionals\", \n",
        "\"difficulty\": \"regular_college\", \n",
        "\"year\": 2018, \"proto_id\": null, \n",
        "\"qdb_id\": 148926, \n",
        "\"dataset\": \"quizdb.org\", \n",
        "\"qanta_id\": 106223, \n",
        "\"tokenizations\": [[0, 154], [155, 272], [272, 273], [274, 412], [413, 577], [578, 700], [701, 802], [803, 916]], \n",
        "\"first_sentence\": \"The lines \\\"Rude, and ungrateful, though my country be,\\\" in Book 5 of this work alludes to Scipio Africanus as a reference to its author's multiple exiles.\", \n",
        "\"gameplay\": false, \n",
        "\"fold\": \"guesstest\"}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJ7TzW3DyFx6"
      },
      "source": [
        "#FINAL CODE FOR TONIGHT\n",
        "#EDGE CASE 1\n",
        "\n",
        "[200369,0] 2000369\t0\t0\tThe_Clue\t\t0\n",
        "[200369,1] 2000369\t1\t1\tSatire\t\t1\n",
        "\n",
        "\n",
        "[200369,2] [200369,3] [200369,4] [200369,5] All answers for remaining sentence numbers will be Satire and all buzz values will be 1.\n",
        "In final.csv you will enter ['200369', 'Satire'].\n",
        "You do not have to call the classify function for the remaining sentences in QUestion 200369 below as you can hardcode as per the heuristics defined.\n",
        "You straight away go to the next Question.\n",
        "\n",
        "2000369\t2\t1\tSatire\t\t1\n",
        "2000369\t3\t13\tSatire\t\t1\n",
        "2000369\t4\t9\tSatire\t\t1\n",
        "2000369\t5\t4\tSatire\t\t1\n",
        "\n",
        "#EDGE CASE 2\n",
        "\n",
        "2000370\t0\t0\tThe_Golden_Palace\t\t0\n",
        "2000370\t1\t17\tPeter_Debye\t\t0\n",
        "2000370\t2\t16\tMaxwell's_demon\t\t0\n",
        "2000370\t3\t2\tJames_Clerk_Maxwell\t\t0\n",
        "\n",
        "WHEN YOU SEE THAT THE BUZZ VALUE IS 0 FOR ALL THE SENTENCES OF THAT QUESTION (SUGGESTION: USE A FLAG VARIABLE FOR EVERY QUESTION. Then\n",
        "\n",
        "1. DO NOT WORRY ABOU BUZZ.CSV. IT IS GETTING FILLED AS IT SHOULD BE.\n",
        "2. BUT FINAL.CSV IS ONLY GETTING FILLED IF WE ARE GOING INTO FINAL_BUZZ==1 CONDITION. SO WE MUST MAKE SURE IN THIS CASE THAT THE ANSWER IN FINAL.CSV WILL BE THE LAST ANSWER IDENTIFIED THAT IS FOR THE ABOVE EXAMPLE, THE ENTRY WOULD BE ['200370','James_Clerk_Maxwell]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8YLeJcnA-jF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "4b13ce90-cbae-41a1-85f8-fc8dd9b8d9b4"
      },
      "source": [
        "#Block 8 (Buzz for answers above a threshold value and generate a csv file adapting knn_hack.py in Block 4)\n",
        "#K-NN Classification code to classify answers from questions\n",
        "%%writefile /content/umd_hackathon2021/knn_hack_thresh_buzzer.py\n",
        "\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import json\n",
        "from collections import Counter, defaultdict\n",
        "from typing import Sequence, Dict\n",
        "\n",
        "import numpy\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import csv\n",
        "\n",
        "class Knearest:\n",
        "    \"\"\"\n",
        "    kNN classifier\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, x, y, k):\n",
        "        \"\"\"\n",
        "        Creates a kNN instance\n",
        "        :param x: Training data input\n",
        "        :param y: Training data output\n",
        "        :param k: The number of nearest points to consider in classification\n",
        "        \"\"\"\n",
        "\n",
        "        # You may modify this code, but you shouldn't need to\n",
        "\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.k = k\n",
        "\n",
        "    def majority(self, item_indices: Sequence[int]) -> str:\n",
        "        \"\"\"Given the indices of training examples, return the majority label.\n",
        "        If there's a tie, return the one that is lexicographically\n",
        "        first (as determined by python sorted function).\n",
        "        :param item_indices: The indices of the k nearest neighbors\n",
        "        (helpfully, this is what's returned by the kneighbors\n",
        "        function.\n",
        "        \"\"\"\n",
        "        assert len(item_indices) == self.k, \"Did not get k inputs\"\n",
        "\n",
        "        # Finish this function to return the most common y value for\n",
        "        # these indices\n",
        "\n",
        "        # Gutama and Chiebuka's code block\n",
        "        \n",
        "        freq = Counter(self.y[index] for index in item_indices)\n",
        "        max_count = freq.most_common(1)[0][1]\n",
        "        output_label = sorted(i for i in freq if freq[i]  == max_count)[0]\n",
        "        '''\n",
        "        y=[1,-1,2,3,5]\n",
        "        output_label = 5\n",
        "        index_output_label = 4\n",
        "        '''\n",
        "        #find the index TASK 3\n",
        "        index_output_label= y.index(output_label)\n",
        "        return output_label, index_output_label\n",
        "        return None\n",
        "\n",
        "    def classify(self, example: numpy.ndarray) -> str:\n",
        "        \"\"\"\n",
        "        Given an example, classify the example.\n",
        "        :param example: A representation of an example in the same\n",
        "        format as training data\n",
        "        \"\"\"\n",
        "        # Finish this function to find the k closest points, query the\n",
        "        # majority function, and return the value.\n",
        "        # Gutama and Chiebuka's code block\n",
        "\n",
        "        possible_similarities=self.x.dot(example.T).T # [1,2,3,4,4] or [[];[];[]]\n",
        "\n",
        "        possible_similarities = (-possible_similarities).toarray()\n",
        "       \n",
        "        ind = numpy.argsort(possible_similarities, axis=1)[:, 0:self.k]\n",
        "\n",
        "        # 1 idea is to take the dot produt value of majority(ind[0]) which is already getting calculated in possible_similarities\n",
        "        \n",
        "        # Return the confidence score also in addition to the label which is majority(ind[0])\n",
        "        output_label, index_output_label = self.majority(ind[0])\n",
        "        confidence_value = possible_similarities[index_output_label]\n",
        "        return output_label, confidence_value\n",
        "\n",
        "    def confusion_matrix(self, test_x: Sequence[str], test_y: Sequence[str]) -> Dict[str, Dict[str, int]]:\n",
        "        \"\"\"\n",
        "        Given a matrix of test examples and labels, compute the confusion\n",
        "        matrixfor the current classifier.  Should return a dictionary of\n",
        "        dictionaries where d[ii][jj] is the number of times an example\n",
        "        with true label ii was labeled as jj.\n",
        "        :param test_x: Test data representation\n",
        "        :param test_y: Test data answers\n",
        "        \"\"\"\n",
        "\n",
        "        # Finish this function to build a dictionary with the\n",
        "        # mislabeled examples.  You'll need to call the classify\n",
        "        # function for each example.\n",
        "\n",
        "        d = defaultdict(dict)\n",
        "\n",
        "        # David and Nimay's code block \n",
        "\n",
        "        index = 0\n",
        "        # values to be stored in the buzz csv file\n",
        "        buzz_rows = ['question',\t'sentence', 'word',\t'page',\t'evidence', 'final',\t'weight'] \n",
        "        # values to be stored in the final csv file\n",
        "        final_rows = ['question','answer']\n",
        "        prev_qid = 0\n",
        "        final_found_flag = 0\n",
        "        past_confidence score = 0\n",
        "        past_output_label = ''\n",
        "        past_id_buzz0 = 0\n",
        "        past_output_label_buzz0 = ''\n",
        "        final_buzz = 1\n",
        "        for tx, ty in zip(test_x, test_y):\n",
        "          ''' THIS IS A FOOD. THIS IS WHAT I EAT IN THE MORNING. THIS IS HEALTHY'''\n",
        "          # LOOP ON EACH OF THE 3 SENTENCES. GET SENTENCE NUMBERS FOR EVERY SENTENCE.\n",
        "          i = 0\n",
        "          # loop over the dictionary for all keys (id, sentence number) https://realpython.com/iterate-through-dictionary-python/#iterating-through-keys\n",
        "          '''\n",
        "          d = {['a','c']:45,['a','c']:45,['a','c']:45,['a','c']:45}\n",
        "        >>> for id, sent_num in tx.keys():\n",
        "        test_vector = tx[id,sent_num]\n",
        "        output_label, confidence_score  = classify(test_vector) \n",
        "          '''\n",
        "          id, sent_num = tx.keys()\n",
        "          test_vector = tx[id,sent_num]\n",
        "          \n",
        "          if (prev_qid != id):\n",
        "              prev_qid = id\n",
        "              final_found_flag = 0\n",
        "              if final_buzz == 0:\n",
        "                 final_rows.append([past_id_buzz0, past_output_label_buzz0])\n",
        "\n",
        "\n",
        "          if (final_found_flag == 0):\n",
        "              test_vector = tx[id,sent_num]\n",
        "              output_label, confidence_score  = classify(test_vector)\n",
        "          else:\n",
        "              #confidence_score = 0 #Assuming that our threshold will never be 0 to loop over remaining sentences in a question after an answer has been buzzed\n",
        "              buzz_rows.append([id, sent_num,0,past_output_label,'',1,past_confidence_score])\n",
        "              continue\n",
        "                                                         \n",
        "          if (confidence_score>=THRESHOLD):\n",
        "                  final_buzz = 1   #TASK 4 Value goes to csv file along with associate confidence_score and sentence_number (which is the iteration number in this current for loop)\n",
        "                  buzz_rows.append([id, sent_num,0,output_label,'',final_buzz,confidence_score])\n",
        "\n",
        "                  # 1. final buzz = 1, therefore, stop here?\n",
        "                  if (final_found_flag != 1):\n",
        "                    final_rows.append([id, output_label])\n",
        "                  final_found_flag = 1\n",
        "                  past_confidence_score = confidence_score\n",
        "                  past_output_label = output_label\n",
        "                  '''\n",
        "                  question_id which is qanta_id that you have to associate as a dictionary from below. \n",
        "                  page value in BUZZ CSV FILE and ANSWERS in FINALS CSV file will be the generated output label.\n",
        "                  '''\n",
        "                  ''' ANOTHER IMPORTANT THING IS ONCE YOUR BUZZ IS 1. keep the answer as the final answer in the FINALS csv file\n",
        "                      If there are remaining sentences in the question; all of their answers will be the buzzed final answer (wiki page equivalent to the output_label \n",
        "                      and all of their buzz values will be 1 FOR THE BUZZ CSV FILE\n",
        "\n",
        "                  '''\n",
        "                  # rows.append([i, confidence_score])\n",
        "          else:\n",
        "                  final_buzz = 0 #TASK 5 Value goes to csv file along with associate confidence_score and sentence_number (which is the iteration number in this current for loop)\n",
        "                  buzz_rows.append([id, sent_num,0,output_label,'',final_buzz,confidence_score])\n",
        "                  past_id_buzz0 = id\n",
        "                  past_output_label_buzz0 = output_label\n",
        "                  '''\n",
        "                  question_id which is qanta_id that you have to associate as a dictionary from below. \n",
        "                  I AM LOOKING INTO HOW WE CAN GET THE OUTPUT WIKI PAGE NAME FROM OUTPUT_LABEL WHICH IS JUST A NUMBER AND CAN BE CHALLENGING TO MAP\n",
        "                  '''\n",
        "                  ''' ANOTHER IMPORTANT THING IS \n",
        "\n",
        "                  If no output label's confidence value is above the threshold, then keep the last answer as the final answer IN THE FINALS CSV FILE EVEN IF buzz will be 0 FOR ALL SENTENCES IN BUZZ CSV FILE\n",
        "                  '''\n",
        "                  # rows.append([i, confidence_score])\n",
        "\n",
        "              # prev = final_buzz\n",
        "              prev_qid = id\n",
        "          if (final_buzz == 0):\n",
        "            final_rows.append([id, output_label])\n",
        "          \n",
        "          with open('UMD_Hackathon.buzz.csv', 'w', newline='') as file:\n",
        "            thewriter = csv.writer(file)\n",
        "            thewriter.writerows(buzz_rows)\n",
        "          \n",
        "          with open('UMD_Hackathon.final.csv', 'w', newline='') as file:\n",
        "            thewriter = csv.writer(file)\n",
        "            thewriter.writerows(final_rows)\n",
        "          \n",
        "          # loop on len(tx); generate an output_label and a confidence score from the classify function for each len(tx) which is each sentence of the queston\n",
        "          # get the confidence score for that output_label\n",
        "          # Confidence score is that I have to take dot product of the question's vector representation with the vector representation \n",
        "          # We check if the returned confidence score is above or below a threshold. BUZZ_THRESHOLD = 0.5 if confidence value is 0.7,buzz = 1; if confidence value is 0.4,buzz = 0\n",
        "          # Once an output label crosses the threshold, make buzz = 1; keep the answer as the final answer; if there are remaining sentences in the question; all of their answers will be the buzzed final answer and all of their buzz values will be 1\n",
        "          # If no output label's confidence value is above the threshold, then keep the last answer as the final answer and buzz will be 0.\n",
        "          # For the csv file, we already have the sentence, final (buzz) and the weight (confidence value) calculated from the above algorithm\n",
        "          # Question ID (qanta_id), Answer has to be needed which is the (page) \n",
        "          # test_y is the wiki page; but we are calculating output_label which is not a wiki page\n",
        "          # FOR NOW, DON'T WORRY ABOUT THE ANSWERS IN THE CSV FILE\n",
        "          output_label = self.classify(tx)\n",
        "          d[ty][output_label] = d[ty].get(output_label, 0) + 1\n",
        "          index += 1\n",
        "          if (index % 100 == 0):\n",
        "            print(\"%i/%i Confusion Matrix\" % (index, test_x.shape[0]))\n",
        "         \n",
        "        return d\n",
        "\n",
        "    @staticmethod\n",
        "    def accuracy(confusion_matrix: Dict[str, Dict[str, int]]) -> float:\n",
        "        \"\"\"Given a confusion matrix, compute the accuracy of the underlying\n",
        "        classifier.\n",
        "        \"\"\"\n",
        "\n",
        "        # Hint: this should give you clues as to how the confusion\n",
        "        # matrix should be structured.\n",
        "\n",
        "        total = 0 \n",
        "        correct = 0\n",
        "        for ii in confusion_matrix:\n",
        "            total += sum(confusion_matrix[ii].values())\n",
        "            correct += confusion_matrix[ii].get(ii, 0)\n",
        "\n",
        "        return float(correct) / float(total)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description='KNN classifier options')\n",
        "    parser.add_argument(\"--root_dir\", help=\"QB Dataset for training\",\n",
        "                        type=str, default='../',\n",
        "                        required=False)\n",
        "    parser.add_argument(\"--train_dataset\", help=\"QB Dataset for training\",\n",
        "                        type=str, default='/content/umd_hackathon2021/qanta.train.json',\n",
        "                        required=False)\n",
        "    parser.add_argument(\"--test_dataset\", help=\"QB Dataset for test\",\n",
        "                        type=str, default='/content/umd_hackathon2021/qanta.dev.json',\n",
        "                        required=False)\n",
        "    parser.add_argument(\"--min_df\", help=\"How many documents must a word appear in to be feature\",\n",
        "                        type=int, default=2)\n",
        "    parser.add_argument(\"--max_df\", help=\"How many docs can words appear in and still be feature\",\n",
        "                        type=float, default=0.9)\n",
        "    parser.add_argument(\"--limit\", help=\"Number of training documents\",\n",
        "                        type=int, default=-1, required=False)\n",
        "    parser.add_argument(\"--max_ngram\", help=\"Max ngram length\", type=int, default=3)\n",
        "    parser.add_argument('--k', type=int, default=3,\n",
        "                        help=\"Number of nearest points to use\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # You should not have to modify any of this code\n",
        "    with open(os.path.join(args.root_dir, args.train_dataset)) as infile:\n",
        "        data = json.load(infile)[\"questions\"]\n",
        "        if args.limit > 0:\n",
        "            data = data[:args.limit]\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(1, args.max_ngram), min_df=args.min_df, max_df=args.max_df).fit(x[\"text\"] for x in data)\n",
        "    train_x = vectorizer.transform(x[\"text\"] for x in data)\n",
        "    train_y = list(x[\"page\"] for x in data)\n",
        "\n",
        "    print(type(train_x))\n",
        "\n",
        "    knn = Knearest(train_x, train_y, args.k)\n",
        "    print(\"Done loading data\")\n",
        "    '''\n",
        "    #David's code but had bugs\n",
        "    with open('new.csv', 'r', newline='') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        given_test = {}\n",
        "        for row in reader:\n",
        "          for header, value in row.items():\n",
        "            given_test[header].append(value)\n",
        "\n",
        "      \n",
        "      new_dict = {}\n",
        "\n",
        "      test_x = vectorizer.transform(x[\"text\"] for x in given_test)\n",
        "      new_dict[(x[\"qid\"], x[\"answer\"], x[\"sent\"], x[\"text\"]): test_x]\n",
        "      test_y = list(x[\"answer\"] for x in given_test)\n",
        "    \n",
        "      confusion_matrix = confusion_matrix(self, new_dict[], test_y)\n",
        "    '''\n",
        "    #Instead of using the test data of qanta.dev.json, we will read a csv file (questions.f1.csv)\n",
        "    \n",
        "    #read UMD_Hackathon.csv\n",
        "    # store every information of id, answer,\tsent, text \n",
        "    # given_test = read the csv file in variable given test\n",
        "    # test_x = vectorizer.transform(x[\"text\"] for x in given_test)\n",
        "    # d[x[\"id\"], x[\"sent_text\"]] = test_x\n",
        "    # test_y = list(x[\"answer\"] for x in given_test)\n",
        "\n",
        "    '''\n",
        "    with open(os.path.join(args.root_dir, args.test_dataset)) as infile:\n",
        "        test = json.load(infile)[\"questions\"][:100]\n",
        "\n",
        "    test_x = vectorizer.transform(x[\"text\"] for x in test)\n",
        "    test_qid = vectorizer.transform(x[\"qanta_id\"] for x in test)\n",
        "    new_dict = {test_qid[x]:test_x[x] for x in range(len(test_qid))}\n",
        "    test_y = list(x[\"page\"] for x in test)\n",
        "    '''\n",
        "\n",
        "\n",
        "    #TASK 1 Question ID will be obtained from Test_QID = x[\"qanta_id\"] for x in test; To consider the vector and the ID together; use a dictionary maybe and pass it accordingly to confusion_matrix; \n",
        "    # IN the dictionary, key will be QUESTION ID; Value will be test_x (which is the vector representation of the QUESTION TEXT)\n",
        "   \n",
        "    dict_test_x = {} \n",
        "    test_y = []\n",
        "    with open('UMD_Hackathon.csv', 'r', newline='') as csvfile: \n",
        "         reader = csv.DictReader(csvfile)\n",
        "         for row in reader: \n",
        "             q_id = row[\"id\"]\n",
        "             sent = row[\"sent\"]\n",
        "             q_text = row[\"text\"]\n",
        "             answer = row[\"answer\"]\n",
        "             dict_test[q_id, sent] = vectorizer.transform(q_text)\n",
        "             test_y.append(answer)\n",
        "\n",
        "    confusion = knn.confusion_matrix(dict_test_x, test_y)\n",
        "    \n",
        "    answers = [x[0] for x in Counter(test_y).most_common(5)]\n",
        "\n",
        "    #confusion = knn.confusion_matrix(test_x, test_y)\n",
        "    guesses = set()\n",
        "    for ii in answers:\n",
        "        for jj in confusion[ii]:\n",
        "            guesses.add(jj)\n",
        "\n",
        "    print(\"\\t\" + \"\\t\".join(str(x) for x in answers))\n",
        "    print(\"\".join([\"-\"] * 90))\n",
        "    for ii in guesses:\n",
        "        print(\"%30s:\\t\" % ii + \"\\t\".join(str(confusion[x].get(ii, 0))\n",
        "                                       for x in answers))\n",
        "    print(\"Accuracy: %f\" % knn.accuracy(confusion))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing /content/umd_hackathon2021/knn_hack_thresh_buzzer.py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-148796389e2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'writefile'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/umd_hackathon2021/knn_hack_thresh_buzzer.py'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n\\nimport argparse\\nimport os\\nimport json\\nfrom collections import Counter, defaultdict\\nfrom typing import Sequence, Dict\\n\\nimport numpy\\nfrom sklearn.neighbors import NearestNeighbors\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nimport csv\\n\\nclass Knearest:\\n    \"\"\"\\n    kNN classifier\\n    \"\"\"\\n\\n    def __init__(self, x, y, k):\\n        \"\"\"\\n        Creates a kNN instance\\n        :param x: Training data input\\n        :param y: Training data output\\n        :param k: The number of nearest points to consider in classification\\n        \"\"\"\\n\\n        # You may modify this code, but you shouldn\\'t need to\\n\\n        self.x = x\\n        self.y = y\\n        self.k = k\\n\\n    def majority(self, item_indices: Sequence[int]) -> str:\\n        \"\"\"Given the indices of training examples, return the majority label.\\n        If there\\'s a tie, return the one that is lexicographically\\n        first (as determined by python sorted function).\\n        :param item_indices: The indices of the k nearest neighbors\\n        (helpfully, this is what\\'s returned by the kneighbors\\n        function.\\n        \"\"\"\\n        assert len(item_indices) == self.k, \"Did not get k inputs\"\\n\\n        # Finish this function to return the most common y val...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-97>\u001b[0m in \u001b[0;36mwritefile\u001b[0;34m(self, line, cell)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/osm.py\u001b[0m in \u001b[0;36mwritefile\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'a'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/umd_hackathon2021/knn_hack_thresh_buzzer.py'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI3e14v6pOen",
        "outputId": "82e15873-8f0f-4402-9692-f856978ecd48"
      },
      "source": [
        "tx = {('a','c'):44,('m','c'):5,('a','g'):455,('d','c'):465}\n",
        "\n",
        "for id, sent_num in tx.keys():\n",
        "  print(tx[id,sent_num])\n",
        "  print(id,sent_num)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "44\n",
            "a c\n",
            "5\n",
            "m c\n",
            "455\n",
            "a g\n",
            "465\n",
            "d c\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}